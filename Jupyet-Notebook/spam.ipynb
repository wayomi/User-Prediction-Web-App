{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"F:\\Data\\Experimental/spam.csv\", encoding = 'latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(labels = [\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_Train = data.v1\n",
    "X_Train = data.drop(columns = 'v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Preprocessing\n",
    "import nltk\n",
    "#nltk.download(\"all\")\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       None\n",
       "1       None\n",
       "3       None\n",
       "4       None\n",
       "6       None\n",
       "        ... \n",
       "5565    None\n",
       "5568    None\n",
       "5569    None\n",
       "5570    None\n",
       "5571    None\n",
       "Name: v2, Length: 4825, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_messages = data[data[\"v1\"] == \"spam\"][\"v2\"]\n",
    "ham_messages = data[data[\"v1\"] == \"ham\"][\"v2\"]\n",
    "\n",
    "spam_words = []\n",
    "ham_words = []\n",
    "\n",
    "def extractSpamWords(spamMessages):\n",
    "    global spam_words\n",
    "    words = [word.lower() for word in word_tokenize(spamMessages) if word.lower() not in stopwords.words(\"english\") and word.lower().isalpha()]\n",
    "    spam_words = spam_words + words\n",
    "    \n",
    "def extractHamWords(hamMessages):\n",
    "    global ham_words\n",
    "    words = [word.lower() for word in word_tokenize(hamMessages) if word.lower() not in stopwords.words(\"english\") and word.lower().isalpha()]\n",
    "    ham_words = ham_words + words\n",
    "\n",
    "spam_messages.apply(extractSpamWords)\n",
    "ham_messages.apply(extractHamWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    5572.000000\n",
       "mean       80.118808\n",
       "std        59.690841\n",
       "min         2.000000\n",
       "25%        36.000000\n",
       "50%        61.000000\n",
       "75%       121.000000\n",
       "max       910.000000\n",
       "Name: messageLength, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"messageLength\"] = data[\"v2\"].apply(len)\n",
    "data[\"messageLength\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>messageLength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>go jurong point crazi avail bugi n great world...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>ok lar joke wif u oni</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>free entri 2 wkli comp win fa cup final tkts 2...</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>u dun say earli hor u c alreadi say</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>nah dont think goe usf live around though</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>freemsg hey darl 3 week word back id like fun ...</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>even brother like speak treat like aid patent</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>per request mell mell oru minnaminungint nurun...</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>winner valu network custom select receivea å£9...</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>mobil 11 month u r entitl updat latest colour ...</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2  messageLength\n",
       "0   ham  go jurong point crazi avail bugi n great world...            111\n",
       "1   ham                              ok lar joke wif u oni             29\n",
       "2  spam  free entri 2 wkli comp win fa cup final tkts 2...            155\n",
       "3   ham                u dun say earli hor u c alreadi say             49\n",
       "4   ham          nah dont think goe usf live around though             61\n",
       "5  spam  freemsg hey darl 3 week word back id like fun ...            148\n",
       "6   ham      even brother like speak treat like aid patent             77\n",
       "7   ham  per request mell mell oru minnaminungint nurun...            160\n",
       "8  spam  winner valu network custom select receivea å£9...            158\n",
       "9  spam  mobil 11 month u r entitl updat latest colour ...            154"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def cleanText(data):\n",
    "    \n",
    "    data = data.translate(str.maketrans('', '', string.punctuation))\n",
    "    words = [stemmer.stem(word) for word in data.split() if word.lower() not in stopwords.words(\"english\")]\n",
    "    \n",
    "    return \" \".join(words)\n",
    "\n",
    "data[\"v2\"] = data[\"v2\"].apply(cleanText)\n",
    "data.head(n = 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 7903)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vec = TfidfVectorizer(encoding = \"latin-1\", strip_accents = \"unicode\", stop_words = \"english\")\n",
    "features = vec.fit_transform(data[\"v2\"])\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodeCategory(cat):\n",
    "    if cat == \"spam\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "        \n",
    "data[\"v1\"] = data[\"v1\"].apply(encodeCategory)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, data[\"v1\"], stratify = data[\"v1\"], test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1115x7903 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 8568 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.940099833610649\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "gaussianNb = MultinomialNB()\n",
    "gaussianNb.fit(X_train, y_train)\n",
    "\n",
    "y_pred = gaussianNb.predict(X_test)\n",
    "\n",
    "print(fbeta_score(y_test, y_pred, beta = 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of NB classifier on test set: 0.97\n"
     ]
    }
   ],
   "source": [
    "predict_y = gaussianNb.predict(X_test)\n",
    "print('Accuracy of NB classifier on test set: {:.2f}'.format(gaussianNb.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "comment = [\"Check this out\"]\n",
    "vect = vec.transform(comment).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaussianNb.predict(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
